<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Maximum Likelihood Reinforcement Learning - A framework that bridges RL and maximum likelihood for correctness-based tasks">
  <meta name="keywords" content="LLM, Reasoning, Reinforcement Learning, Maximum Likelihood, Policy Gradient">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MaxRL: Maximum Likelihood Reinforcement Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    :root {
      --primary: #363636;
      --primary-light: #4a4a4a;
      --text-dark: #1f2937;
      --text-muted: #6b7280;
      --bg-white: #ffffff;
      --bg-gray: #f9fafb;
      --border-light: #e5e7eb;
    }
    
    body { color: var(--text-dark); }
    
    .hero { background: #f3f4f6; border-bottom: 1px solid #e5e7eb; overflow: visible !important; }
    .hero .hero-body { padding-top: 2rem; padding-bottom: 0.8rem; overflow: visible !important; }
    .hero * { overflow: visible !important; }
    .hero .container, .hero .columns, .hero .column { overflow: visible !important; max-height: none !important; }
    .hero .title.is-1 { color: #ab1727; font-weight: 400; font-family: Georgia, 'Times New Roman', serif; }
    .hero .publication-authors a { color: #374151 !important; }
    .hero .publication-authors a:hover { color: #C41230 !important; text-decoration: underline; }
    .hero .publication-authors span { color: #4b5563; }
    
    .publication-links .button {
      margin: 0.25rem;
      background: transparent;
      border: 1px solid #d1d5db;
      color: #374151;
    }
    .publication-links .button:hover {
      background: #f3f4f6;
      border-color: #9ca3af;
      color: #1f2937;
    }

    /* Section titles */
    h2.section-title,
    h3.section-title,
    h4.subsection-title,
    .title.section-title {
      color: var(--primary) !important;
      font-weight: 600;
      font-family: 'Google Sans', sans-serif;
      margin-bottom: 1.5rem;
    }
    
    h4.subsection-title {
      font-size: 1.25rem;
      font-family: 'Google Sans', sans-serif;
      margin-top: 2rem;
      margin-bottom: 1rem;
    }
    
    /* Body text */
    .content p, .content li {
      font-family: 'Noto Sans', sans-serif;
      font-size: 1rem;
      line-height: 1.7;
    }
    
    .section-white { background: var(--bg-white); }
    .section-gray { background: var(--bg-gray); }
    
    .insight-box {
      background: #fafafa;
      border: 1px solid var(--border-light);
      border-left: 4px solid #9ca3af;
      padding: 1.5rem 2rem;
      border-radius: 4px;
      margin: 1.5rem 0;
    }
    .insight-box p {
      font-family: 'Castoro', Georgia, serif;
      font-size: 1.1rem;
    }
    
    .math-comparison {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1.5rem;
      margin: 2rem 0;
    }
    @media (max-width: 768px) { .math-comparison { grid-template-columns: 1fr; } }
    
    .math-box {
      background: #fff;
      border-radius: 6px;
      padding: 1.5rem;
      border: 1px solid var(--border-light);
    }
    .math-box.rl { border-top: 3px solid #dc2626; }
    .math-box.ml { border-top: 3px solid var(--primary); }
    .math-box h4 { font-weight: 600; margin-bottom: 1rem; color: var(--text-dark); }
    
    .figure-container {
      background: transparent;
      border-radius: 0;
      padding: 1rem 0;
      border: none;
      margin: 1.5rem 0;
    }
    .figure-container img { border-radius: 4px; }
    .figure-caption { font-size: 0.9rem; color: var(--text-muted); margin-top: 1rem; text-align: center; }
    
    /* Equal height for analysis figures */
    .analysis-columns {
      display: flex;
      gap: 1.5rem;
      align-items: stretch;
    }
    .analysis-columns .analysis-col {
      flex: 1;
      display: flex;
    }
    .analysis-columns .figure-container {
      flex: 1;
      display: flex;
      flex-direction: column;
      margin: 0;
    }
    .analysis-columns .figure-container .img-wrapper {
      flex: 1;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    .analysis-columns .figure-container img {
      max-width: 100%;
      max-height: 280px;
      object-fit: contain;
    }
    @media (max-width: 768px) {
      .analysis-columns { flex-direction: column; }
    }
    
    .result-card {
      background: #fff;
      border-radius: 8px;
      padding: 2rem;
      margin: 1.5rem 0;
      border: 1px solid var(--border-light);
    }
    .result-card h4 { color: var(--text-dark); font-weight: 600; margin-bottom: 1rem; }
    
    .highlight-stat {
      background: var(--primary);
      color: #fff;
      padding: 0.2rem 0.6rem;
      border-radius: 4px;
      font-weight: 600;
      font-size: 0.9rem;
    }
    
    .algorithm-box {
      background: #1f2937;
      color: #e5e7eb;
      border-radius: 6px;
      padding: 1.5rem;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.85rem;
      overflow-x: auto;
    }
    .algorithm-box .comment { color: #9ca3af; }
    .algorithm-box .keyword { color: #f472b6; }
    .algorithm-box .function { color: #60a5fa; }
    
    table.comparison-table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      background: #fff;
      border: 1px solid var(--border-light);
      border-radius: 6px;
      overflow: hidden;
    }
    table.comparison-table th {
      background: var(--primary);
      color: #fff !important;
      padding: 0.75rem 1rem;
      font-weight: 600;
      text-align: left;
    }
    table.comparison-table th * { color: #fff !important; }
    table.comparison-table th .katex, table.comparison-table th .katex * { color: #fff !important; }
    table.comparison-table td { padding: 0.75rem 1rem; border-bottom: 1px solid var(--border-light); }
    table.comparison-table tr:last-child td { border-bottom: none; }
    table.comparison-table tr:hover td { background: var(--bg-gray); }
    
    pre code { display: block; padding: 1rem; background: #1f2937; color: #e5e7eb; border-radius: 6px; overflow-x: auto; }
    .bibtex-box { 
      background: #374151;
      border: 1px solid #4b5563;
      border-radius: 8px; 
      padding: 1.5rem;
    }
    .bibtex-box pre {
      margin: 0;
      background: transparent;
    }
    .bibtex-box code {
      font-family: 'JetBrains Mono', 'Fira Code', 'Monaco', monospace;
      font-size: 0.85rem;
      line-height: 1.6;
      color: #e5e7eb;
      background: transparent;
      padding: 0;
    }
    
    footer.footer { background: #1f2937; color: #9ca3af; padding: 2rem; }
    footer a { color: #93c5fd; }
    footer a:hover { color: #bfdbfe; }
    
    .emoji-icon { margin-right: 0.5rem; }
    .section { padding: 3rem 1.5rem; }
    
    .formula-box {
      background: transparent;
      border: none;
      padding: 1rem 0;
      margin: 1.5rem 0;
      text-align: center;
    }
    
    
    .conclusion-box {
      background: #fff;
      border: 1px solid var(--border-light);
      border-radius: 8px;
      padding: 2rem;
    }
    .conclusion-box p {
      line-height: 1.8;
      margin-bottom: 1rem;
    }
    .conclusion-box p:last-child {
      margin-bottom: 0;
    }
    
    /* ==================== MOBILE RESPONSIVE STYLES ==================== */
    
    /* Tablets and smaller */
    @media (max-width: 1024px) {
      .hero .title.is-1 { font-size: 2.2rem; }
    }
    
    /* Mobile devices */
    @media (max-width: 768px) {
      /* Hero Section */
      .hero .hero-body { padding-top: 1.2rem; padding-bottom: 0.6rem; }
      .hero .title.is-1 { 
        font-size: 1.5rem; 
        white-space: normal !important; 
        line-height: 1.3;
      }
      
      /* Authors - make them wrap nicely */
      .publication-authors { font-size: 0.9rem !important; }
      .publication-authors .author-block { 
        display: inline; 
        white-space: nowrap;
      }
      .is-size-6.publication-authors { font-size: 0.8rem !important; }
      .is-size-6.publication-authors .author-block { margin-left: 0.3em !important; }
      
      /* Section padding */
      .section { padding: 2rem 1rem; }
      
      /* Titles */
      h2.title.is-3, .title.is-3.section-title { font-size: 1.4rem; }
      h4.subsection-title { font-size: 1.1rem; }
      
      /* Math comparison boxes */
      .math-comparison { gap: 1rem; }
      .math-box { padding: 1rem; }
      .math-box p { font-size: 0.95rem; }
      
      /* Formula boxes - handle overflow */
      .formula-box { 
        overflow-x: auto; 
        padding: 0.5rem;
      }
      .formula-box p { font-size: 0.9rem; }
      
      /* Insight boxes */
      .insight-box { padding: 1rem 1.25rem; }
      .insight-box p { font-size: 1rem; }
      
      /* Result cards */
      .result-card { padding: 1.25rem; }
      .result-card h4 { font-size: 1.1rem; }
      
      /* Figure captions */
      .figure-caption { font-size: 0.8rem; }
      
      /* Tables - make scrollable */
      .comparison-table-wrapper {
        overflow-x: auto;
        -webkit-overflow-scrolling: touch;
      }
      table.comparison-table { 
        min-width: 500px; 
        font-size: 0.85rem;
      }
      table.comparison-table th, 
      table.comparison-table td { 
        padding: 0.5rem 0.75rem; 
      }
      
      /* Conclusion box */
      .conclusion-box { padding: 1.25rem; }
      
      /* BibTeX */
      .bibtex-box { padding: 1rem; }
      .bibtex-box code { font-size: 0.7rem; }
      
      /* Publication links buttons */
      .publication-links .button { 
        font-size: 0.85rem;
        padding: 0.4rem 0.8rem;
      }
      
      /* Footer */
      footer.footer { padding: 1.5rem 1rem; }
      footer.footer img { height: 60px !important; }
      footer p { font-size: 0.85rem; }
      
      /* TL;DR text */
      .hero .hero-body p[style*="Castoro"] { font-size: 0.95rem !important; }
    }
    
    /* Small phones */
    @media (max-width: 480px) {
      .hero .title.is-1 { font-size: 1.25rem; }
      .publication-authors { font-size: 0.8rem !important; }
      
      h2.title.is-3, .title.is-3.section-title { font-size: 1.25rem; }
      
      .content p, .content li { font-size: 0.9rem; }
      
      .math-box h4 { font-size: 0.95rem; }
      .math-box p { font-size: 0.85rem; }
      
      .result-card { padding: 1rem; }
      .result-card h4 { font-size: 1rem; }
      
      /* Extra small formulas */
      .formula-box p { font-size: 0.85rem; }
      
      /* Theorem boxes */
      .result-card[style*="border-left"] { padding: 1rem; }
      .result-card[style*="border-left"] h4 { font-size: 1rem; }
      .result-card[style*="border-left"] div[style*="text-align: center"] { font-size: 0.9rem !important; }
    }
    
    /* Ensure images never overflow */
    img { max-width: 100%; height: auto; }
    
    /* KaTeX formula overflow handling */
    .katex-display { overflow-x: auto; overflow-y: hidden; }
    
    /* Global overflow prevention */
    .container { overflow-x: hidden; }
    .content { word-wrap: break-word; overflow-wrap: break-word; }
    
    /* Table wrapper for mobile scrolling */
    .comparison-table-wrapper {
      width: 100%;
      overflow-x: auto;
      -webkit-overflow-scrolling: touch;
    }
    
    /* Algorithm box mobile */
    @media (max-width: 768px) {
      .algorithm-box { 
        font-size: 0.75rem; 
        padding: 1rem;
      }
    }
    
    /* Result card theorem boxes mobile adjustments */
    @media (max-width: 768px) {
      .result-card div[style*="text-align: center"] {
        overflow-x: auto;
        -webkit-overflow-scrolling: touch;
      }
    }
  </style>
</head>
<body>

<!-- Hero Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Maximum Likelihood Reinforcement Learning</h1>
          
          <!-- Authors Section -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://tajwarfahim.github.io/">Fahim Tajwar</a><sup>*,1</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=SU6ooAQAAAAJ">Guanning Zeng</a><sup>*,2</sup>,</span>
            <span class="author-block"><a href="https://zhouyueer7.github.io/">Yueer Zhou</a><sup>3</sup>,</span>
            <span class="author-block"><a href="https://yudasong.github.io/">Yuda Song</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://daman1209arora.github.io/">Daman Arora</a><sup>1</sup>,</span><br>
            <span class="author-block"><a href="https://yidingjiang.github.io/">Yiding Jiang</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://www.cs.cmu.edu/~schneide/">Jeff Schneider</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://havenfeng.github.io/">Haiwen Feng</a><sup>4,5</sup>,</span>
            <span class="author-block"><a href="https://azanette.com">Andrea Zanette</a><sup>1</sup></span>
          </div>
          <div class="is-size-6 publication-authors" style="margin-top: 0.4rem;">
            <span class="author-block"><sup>1</sup>CMU</span>
            <span class="author-block" style="margin-left: 0.8em;"><sup>2</sup>Tsinghua University</span>
            <span class="author-block" style="margin-left: 0.8em;"><sup>3</sup>Zhejiang University</span>
            <span class="author-block" style="margin-left: 0.8em;"><sup>4</sup>UC Berkeley</span>
            <span class="author-block" style="margin-left: 0.8em;"><sup>5</sup>Impossible, Inc.</span>
            <span class="author-block" style="margin-left: 0.8em;"><sup>*</sup>Equal Contribution</span>
          </div>

          <div class="has-text-centered" style="margin-top: 0.8rem;">
            <div class="publication-links">
              <span class="link-block">
                <a href="./static/pdf/MaxRL_Preprint.pdf" class="external-link button is-normal is-rounded" target="_blank">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/tajwarfahim/maxrl" class="external-link button is-normal is-rounded">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>

          <div style="margin-top: 0.6rem; text-align: center;">
            <p style="font-family: 'Castoro', Georgia, serif; font-size: 1.05rem; color: #374151;">
              <strong>TL;DR</strong>&nbsp; A framework to optimize maximum likelihood with reinforcement learning.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ==================== ABSTRACT ==================== -->
<section class="section section-white" style="padding-top: 0.8rem; padding-bottom: 0.5rem;">
  <div class="container is-max-desktop">
    
    <!-- Teaser Diagram -->
    <div class="figure-container" style="margin: 0;">
      <center>
        <img src="./static/images/teaser3.png" alt="MaxRL Teaser" style="max-width: 100%;">
      </center>
    </div>
    
    <!-- Qwen3-4B Results Figure -->
    <div class="figure-container" style="margin-top: 0.5rem;">
      <center>
        <img src="./static/images/large_scale_experiments_summary_qwen_3_4B.png" alt="MaxRL Results on Qwen3-4B" style="max-width: 100%;">
      </center>
      <p class="figure-caption">
        <strong>Large-scale results on Qwen3-4B.</strong> MaxRL Pareto-dominates GRPO across all benchmarks, achieving higher Pass@1 while simultaneously improving Pass@K. This translates to <strong>2.3×–19.2×</strong> gains at test-time scaling efficiency.
      </p>
    </div>
    
    <div class="content has-text-justified" style="margin-top: 1.5rem;">
      <p>
        We introduce <strong>Maximum Likelihood Reinforcement Learning (MaxRL)</strong>, a framework to optimize maximum likelihood with reinforcement learning. 
      </p>
    </div>
  </div>
</section>

<!-- ==================== RL VS ML ==================== -->
<section class="section section-white">
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      
      <h2 class="title is-3 section-title">Why Maximum Likelihood?</h2>
      
      <p>
        Maximum likelihood emerges as a principled objective in supervised learning, as it has reliably translated increases in model capacity, data, and compute into consistent performance improvements. 
        In contrast, many modern learning problems such as code generation, mathematical reasoning, and multi-step planning involve non-differentiable generation but admit an implicit <strong>binary notion of correctness</strong>. 
        For each input $x$, the model induces a probability of success $p_\theta(x) = p_\theta(y^* | x)$ over the correct answer $y^*$, defining an implicit likelihood over correct outcomes.
      </p>
      
      <p>
        In these settings, <strong>reinforcement learning</strong> is typically applied. 
        RL was conceived to handle problems where the likelihood cannot be optimized directly due to non-differentiable intermediate sampling. 
        However, the two approaches optimize <strong>fundamentally different objectives</strong>:
      </p>
      
      <div class="math-comparison">
        <div class="math-box" style="border-top: 3px solid #4b5563;">
          <h4 style="color: #374151;">Reinforcement Learning</h4>
          <p style="text-align: center; font-size: 1.1rem;">
            $\nabla_\theta J_{\mathrm{RL}} = \mathbb{E}_x\left[\nabla_\theta p_\theta(x)\right]$
          </p>
          <p style="color: var(--text-muted); font-size: 0.9rem; margin-top: 1rem; text-align: center;">
            Maximizes expected correctness (pass@1)
          </p>
        </div>
        <div class="math-box" style="border-top: 3px solid #4b5563;">
          <h4 style="color: #374151;">Maximum Likelihood</h4>
          <p style="text-align: center; font-size: 1.1rem;">
            $\nabla_\theta J_{\mathrm{ML}} = \mathbb{E}_x\left[\nabla_\theta \log p_\theta(x)\right]$
          </p>
          <p style="color: var(--text-muted); font-size: 0.9rem; margin-top: 1rem; text-align: center;">
            Reweights by inverse success probability
          </p>
        </div>
      </div>
      
      <p>
        Taking the gradient of the log introduces a $1/p_\theta(x)$ factor, which places greater emphasis on hard, low-success inputs. 
        This leads to very different optimization dynamics: maximum likelihood pushes learning signal toward difficult problems where the model struggles.
      </p>
      
      <p>
        Nevertheless, maximum likelihood is statistically challenging to estimate when $p_\theta(x)$ is small. 
        <strong>Can we approximate maximum likelihood in a way that scales with compute?</strong>
      </p>

    </div>
  </div>
</section>

<!-- ==================== MAXRL METHOD ==================== -->
<section class="section section-white">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">MaxRL: Approximating Maximum Likelihood with More Compute</h2>
    
    <div class="content has-text-justified">
      
      <p>
        We show that the challenge of estimating maximum likelihood admits a principled resolution that <strong>scales with compute</strong>. 
        The key insight comes from a Maclaurin expansion of the log-likelihood.
      </p>
      
      <!-- Maclaurin Expansion -->
      <h4 class="subsection-title">Maclaurin Expansion of Maximum Likelihood</h4>
      
      <p>The log-likelihood admits a <strong>Maclaurin expansion</strong> in terms of failure events:</p>
      
      <div class="formula-box">
        <p style="font-size: 1rem; margin: 0;">
          $J_{\mathrm{ML}}(x) = \log p = -\sum_{k=1}^{\infty}\frac{(1-p)^k}{k} = -\sum_{k=1}^{\infty}\frac{\mathrm{fail@}k(x)}{k}$
        </p>
      </div>
      
      <p>Differentiating yields the <strong>population-level gradient identity</strong>:</p>
      
      <div class="formula-box">
        <p style="font-size: 1.1rem; margin: 0;">
          $\nabla_\theta J_{\mathrm{ML}}(x) = \sum_{k=1}^{\infty}\frac{1}{k}\,\nabla_\theta \mathrm{pass@}k(x)$
        </p>
      </div>
      
      <p>
        From this Maclaurin expansion, we see that <strong>maximum likelihood optimizes an infinite harmonic mixture of pass@k gradients</strong>. 
        Higher-order terms encode learning signal from increasingly rare success patterns—critical when the pass rate $p$ is small.
      </p>
      
      <p>
        In contrast, standard RL optimizes only $\nabla_\theta \mathrm{pass@}1(x)$—the <strong>first-order term</strong> of this expansion:
      </p>
      
      <div class="insight-box">
        <p style="margin: 0;">
          Reinforcement learning is a first-order approximation of maximum likelihood.
        </p>
      </div>
      
      <!-- MaxRL Objective -->
      <h4 class="subsection-title">MaxRL Objective Function</h4>
      
      <p>
        Optimizing the full infinite mixture is infeasible. We define the <strong>truncated maximum likelihood objective</strong> at level $T$:
      </p>
      
      <div class="formula-box">
        <p style="font-size: 1rem; margin: 0;">
          $\nabla_\theta J_{\mathrm{MaxRL}}^{(T)}(x) = \sum_{k=1}^{T}\frac{1}{k}\,\nabla_\theta \mathrm{pass@}k(x)$
        </p>
      </div>
      
      <p>This defines a <strong>compute-indexed family of objectives</strong>:</p>
      

      <div style="margin: 1.5rem 0;">
        <center>
          <iframe
            src="./weight-function.html"
            title="MaxRL Weight Functions"
            width="1000"
            height="850"
            style="border: none; overflow: hidden;"
            scrolling="no">
          </iframe>
        </center>
      </div>
          
      <div class="insight-box">
        <p style="margin: 0;">
          MaxRL objectives provide a principled framework for trading additional compute for higher-fidelity approximations to maximum likelihood.
        </p>
      </div>
      
    </div>
  </div>
</section>

<!-- ==================== PRACTICAL GRADIENT ESTIMATOR ==================== -->
<section class="section section-gray">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Practical Gradient Estimator of MaxRL</h2>
    
    <div class="content has-text-justified">
      
      <p>
        A natural approach to estimate the truncated objective is to approximate each pass@k term separately. 
        We take an alternate approach that leads to a simpler estimator and a new viewpoint.
      </p>
      
      <!-- Theorem 1 -->
      <div class="result-card" style="border-left: 4px solid var(--primary); margin: 1.5rem 0;">
        <h4 style="color: var(--primary); margin-bottom: 0.8rem;">Theorem 1: Conditional Form of the ML Gradient</h4>
        <p style="margin-bottom: 1rem;">The gradient of the maximum likelihood objective admits the following conditional expectation representation:</p>
        <div style="text-align: center; font-size: 1.1rem; padding: 0.5rem 0;">
          $\nabla_\theta J_{\mathrm{ML}}(x) = \mathbb{E}\left[\nabla_\theta \log \pi_\theta(z \mid x) \;\middle|\; \text{success}\right]$
        </div>
        <p style="color: var(--text-muted); font-size: 0.9rem; margin-top: 1rem; margin-bottom: 0;">
          <strong>Interpretation:</strong> The ML gradient equals the average score function over <em>successful trajectories only</em>.
        </p>
      </div>
      
      <p>
        This theorem suggests a simple estimator: sample $N$ trajectories from the policy, then average the score functions only over successful ones.
        Given $N$ rollouts with $K$ successes, we define:
      </p>
      
      <div class="formula-box" style="margin: 1.5rem 0;">
        <p style="font-size: 1rem; margin: 0;">
          $\widehat{g}_N(x) = \begin{cases} \displaystyle\frac{1}{K}\sum_{i=1}^N r_i S_i, & K \ge 1 \\[0.6em] 0, & K = 0 \end{cases}$
        </p>
      </div>
      
      <p>
        where $r_i \in \{0,1\}$ is the binary reward and $S_i = \nabla_\theta \log \pi_\theta(z_i \mid x)$ is the score function.
      </p>
      
      <!-- Theorem 2 -->
      <div class="result-card" style="border-left: 4px solid var(--primary); margin: 1.5rem 0;">
        <h4 style="color: var(--primary); margin-bottom: 0.8rem;">Theorem 2: Estimator–Objective Equivalence</h4>
        <p style="margin-bottom: 1rem;">The estimator $\widehat{g}_N(x)$ is an unbiased estimator for the MaxRL gradient of order $T = N$:</p>
        <div style="text-align: center; font-size: 1.1rem; padding: 0.5rem 0;">
          $\mathbb{E}\left[\widehat{g}_N(x)\right] = \nabla_\theta J_{\mathrm{MaxRL}}^{(N)}(x)$
        </div>
        <p style="color: var(--text-muted); font-size: 0.9rem; margin-top: 1rem; margin-bottom: 0;">
          <strong>Implication:</strong> Using $N$ rollouts automatically targets the $T=N$ truncated ML objective—no explicit pass@k estimation needed.
        </p>
      </div>
            
      <p>The difference between REINFORCE and MaxRL is remarkably simple at the estimator level:</p>
      
      <div class="math-comparison">
        <div class="math-box" style="border-top: 3px solid #1e40af;">
          <h4 style="color: #1e40af;">REINFORCE</h4>
          <p style="text-align: center; font-size: 1.1rem;">
            $\displaystyle\frac{1}{N}\sum_{i=1}^N r_i S_i$
          </p>
          <p style="color: var(--text-muted); font-size: 0.9rem; margin-top: 1rem;">
            <strong>Unbiased for:</strong> $\nabla_\theta\,\mathrm{pass@}1$
            <br><br>
            <strong>→ Normalize by total samples $N$</strong>
          </p>
        </div>
        <div class="math-box" style="border-top: 3px solid #C41230;">
          <h4 style="color: #C41230;">MaxRL (Ours)</h4>
          <p style="text-align: center; font-size: 1.1rem;">
            $\displaystyle\frac{1}{K}\sum_{i=1}^N r_i S_i$
          </p>
          <p style="color: var(--text-muted); font-size: 0.9rem; margin-top: 1rem;">
            <strong>Unbiased for:</strong> $\sum_{k=1}^{N}\frac{1}{k}\nabla_\theta \mathrm{pass@}k$
            <br><br>
            <strong>→ Normalize by successful samples $K$</strong>
          </p>
        </div>
      </div>
      
      <div class="insight-box">
        <p style="margin: 0;">
          Increasing $N$ in REINFORCE reduces <em>variance</em> of a fixed pass@1 objective. 
          In MaxRL, increasing $N$ improves the <em>objective itself</em>, approaching maximum likelihood.
        </p>
      </div>
      
      <!-- Variance Reduction -->
      <h4 class="subsection-title">Variance Reduction</h4>
      
      <p>
        We reduce variance using a zero-mean control variate—the unconditional average score $V_N = \frac{1}{N}\sum_{i=1}^N S_i$, which satisfies $\mathbb{E}[V_N]=0$. 
        Subtracting $V_N$ preserves unbiasedness while reducing variance.
        The on-policy implementation differs from standard REINFORCE by a <strong>single-line modification</strong> to the advantage calculation, 
        where the advantage is normalized by the per-task mean reward $\hat{r}$, rather than left unnormalized (RLOO) or normalized by standard deviation (GRPO):
      </p>
      
           <div style="margin: 1.5rem 0;">
            <center>
              <iframe
                src="./code.html"
                title="MaxRL Advantage Code Diff"
                width="900"
                height="905"
                style="border: none; overflow: hidden;"
                scrolling="no">
              </iframe>
            </center>
          </div>
          
      
      <!-- Comparison with GRPO -->
      <h4 class="subsection-title">Comparison with GRPO</h4>
      
      <p>
        All methods—RL, GRPO, MaxRL, ML—admit population-level gradients of the form 
        $\nabla_\theta J = \mathbb{E}_{x}\left[w(p_\theta(x)) \nabla_\theta p_\theta(x)\right]$, 
        where $w(p)$ determines how learning signal is allocated across inputs of varying difficulty:
      </p>
      
      <div class="comparison-table-wrapper">
        <table class="comparison-table" style="margin: 1.5rem 0;">
          <thead>
            <tr>
              <th>Method</th>
              <th>Weight $w(p)$</th>
              <th>Emphasis</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>RL (REINFORCE)</strong></td>
              <td style="text-align: center;">$1$</td>
              <td>Uniform across difficulties</td>
            </tr>
            <tr>
              <td><strong>GRPO</strong></td>
              <td style="text-align: center;">$\frac{1}{\sqrt{p(1-p)}}$</td>
              <td>Moderate upweighting of hard inputs</td>
            </tr>
            <tr style="background: linear-gradient(135deg, #fef2f2 0%, #fee2e2 100%);">
              <td><strong>MaxRL ($T$)</strong></td>
              <td style="text-align: center;">$\frac{1-(1-p)^T}{p}$</td>
              <td>Approaches ML as $T \to \infty$</td>
            </tr>
            <tr>
              <td><strong>Maximum Likelihood</strong></td>
              <td style="text-align: center;">$\frac{1}{p}$</td>
              <td>Strong emphasis on hard inputs</td>
            </tr>
          </tbody>
        </table>
      </div>
      
      <p>
        GRPO's $1/\sqrt{p(1-p)}$ weighting provides moderate upweighting of hard inputs, which is a heuristic choice but contributes to its superior performance compared with vanilla REINFORCE.
        However, GRPO assigns <em>increased</em> weight to very easy inputs ($p \to 1$), unlike likelihood-based objectives.
        In contrast, MaxRL's normalization by $K$ enables approximation to maximum likelihood with respect to the number of rollouts.
      </p>
      
    </div>
  </div>
</section>

<!-- ==================== EXPERIMENTS ==================== -->
<section class="section section-gray">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Experiments</h2>
    
    <div class="content has-text-justified" style="margin-bottom: 2rem;">
      <p>
        We first show that MaxRL closely approximates exact maximum likelihood where it is computable on a toy image classification task, 
        and then demonstrate consistent improvements across maze navigation, GSM8K math reasoning, 
        and finally on large-scale Qwen3 training and challenging math reasoning problems.
      </p>
    </div>
    
    <div class="result-card">
      <h4>ImageNet: Comparison with Exact Likelihood</h4>
      <p>
        We first validate MaxRL where exact maximum likelihood (cross-entropy) is computable. 
        Image classification provides a clean testbed: reward is 1 if predicted class matches ground truth, 0 otherwise.
      </p>
      <div class="figure-container">
        <center><img src="./static/images/imagenet_new_main_paper_figure_1.png" alt="ImageNet results" style="max-width: 80%;"></center>
        <p class="figure-caption">
          <strong>ImageNet training dynamics.</strong> With sufficient rollouts, MaxRL closely matches cross-entropy training, 
          while REINFORCE fails to make progress from low initial pass rates.
        </p>
      </div>
    </div>
    
    <div class="result-card">
      <h4>Maze Navigation: Infinite Data Regime</h4>
      <p>
        We study training with continually fresh data using procedurally generated mazes. 
        Each training input is newly generated, and the model never encounters the same maze twice.
      </p>
      <div class="figure-container">
        <center><img src="./static/images/maze_scaling_with_number_of_rollouts.png" alt="Maze scaling results" style="max-width: 100%;"></center>
        <p class="figure-caption">
          <strong>Scaling behavior with increasing rollouts per prompt.</strong> MaxRL consistently outperforms GRPO, which outperforms RLOO.
        </p>
      </div>
      <center>
        <img src="./static/images/maze-example-dual.png" alt="Maze visualization" style="max-width: 50%; margin-top: 1rem;">
        <p class="figure-caption" style="margin-top: 0.5rem;">Example maze: successful navigation (left) vs. failure case (right).</p>
      </center>
    </div>
    
    <div class="result-card">
      <h4>GSM8K: Data-Scarce Regime</h4>
      <p>
        In the data-scarce regime, models train for many epochs over a fixed dataset. 
        This exposes differences in how objectives allocate learning signal under repeated training.
      </p>
      <div class="figure-container">
        <center><img src="./static/images/smollm_training_step.png" alt="GSM8K training dynamics" style="max-width: 100%;"></center>
        <p class="figure-caption">
          <strong>Training dynamics on GSM8K.</strong> MaxRL shows slower initial gains but sustained improvement, 
          with substantially less pass@k degradation.
        </p>
      </div>
    </div>
    
    <div class="result-card">
      <h4>Large-Scale LLM Training</h4>
      <p>
        We train <strong>Qwen3-1.7B-Base</strong> and <strong>Qwen3-4B-Base</strong> models on POLARIS-53K (~50K math reasoning prompts), 
        and evaluate on AIME 2025, BeyondAIME, MATH-500, and Minerva.
      </p>
      <div class="figure-container">
        <center><img src="./static/images/large_scale_experiments_summary.png" alt="Large scale LLM results" style="max-width: 100%;"></center>
        <p class="figure-caption">
          <strong>Evaluation on math benchmarks.</strong> MaxRL consistently Pareto dominates GRPO: 
          higher pass@1 <em>and</em> improved pass@k. Improved coverage means achieving the same pass@k 
          requires <strong>2.3× – 19.2×</strong> fewer samples than GRPO.
        </p>
      </div>
    </div>
    
      </div>
</section>

<!-- ==================== CONCLUSION ==================== -->
<section class="section section-white">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Conclusion</h2>
    
    <div class="conclusion-box content has-text-justified">
      <p>
        We introduce <strong>Maximum Likelihood Reinforcement Learning (MaxRL)</strong>, a framework that bridges 
        the gap between reinforcement learning and maximum likelihood for correctness-based tasks. Through a Maclaurin expansion 
        of the log-likelihood, we show that standard RL optimizes only the first-order term (pass@1), while maximum likelihood 
        corresponds to an infinite harmonic mixture of pass@k objectives.
      </p>
      <p>
        MaxRL provides a practical middle ground: by truncating the expansion at level $T = N$ (the number of rollouts), 
        we obtain a compute-indexed family of objectives that progressively approaches maximum likelihood as compute increases. 
        The resulting gradient estimator is remarkably simple—normalizing by the number of successes rather than total samples—yet 
        yields consistent improvements across diverse domains.
      </p>
      <p>
        Empirically, MaxRL demonstrates superior performance in image classification, maze navigation, math reasoning, and 
        large-scale LLM training. Compared with GRPO, MaxRL achieves <strong>higher pass@k while improving pass@1</strong>, requiring <strong>2.3x-19.2x</strong> fewer samples at deployment time to achieve the same pass@k coverage.
      </p>
    </div>
    
  </div>
</section>

<!-- ==================== BIBTEX ==================== -->
<section class="section section-gray" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 section-title">BibTeX</h2>
    <div class="bibtex-box">
      <pre><code>@article{tajwar2025maxrl,
  title={Maximum Likelihood Reinforcement Learning},
  author={Tajwar, Fahim and Zeng, Guanning and Zhou, Yueer and Song, Yuda and 
          Arora, Daman and Jiang, Yiding and Schneider, Jeff and 
          Salakhutdinov, Ruslan and Feng, Haiwen and Zanette, Andrea},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
    </div>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered" style="margin-bottom: 1.5rem;">
      <a href="https://www.cmu.edu/" target="_blank">
        <img src="./static/images/cmu_logo.png" alt="Carnegie Mellon University" style="height: 100px; border-radius: 4px;">
      </a>
    </div>
    <div class="columns is-centered">
      <p>
        Corresponding Authors: <a href="mailto:ftajwar@andrew.cmu.edu">Fahim Tajwar</a>, <a href="mailto:azanette@andrew.cmu.edu">Andrea Zanette</a><br>
        Template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
